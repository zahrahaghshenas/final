{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2F4_v3gXcEy",
        "outputId": "115eb085-a534-48ae-c76e-aa554c9835a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dgl in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "ZoT32Zh4VLqm"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl.function as fn\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "from torch.utils.data import DataLoader\n",
        "import cloudpickle\n",
        "from dgl.nn import GraphConv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set Path"
      ],
      "metadata": {
        "id": "luohAG7wWDYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this segment involves directory and file operations. It creates directories, deletes directories, and extracts the contents of a ZIP file."
      ],
      "metadata": {
        "id": "jIEkyJoAcBdQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "f-_ebl7EFTM-"
      },
      "outputs": [],
      "source": [
        "current_dir = \"./\"\n",
        "checkpoint_path = current_dir + \"save_models/model_checkpoints/\" + \"checkpoint\"\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "best_model_path = current_dir + \"save_models/best_model/\"\n",
        "\n",
        "folder_data_temp = current_dir +\"data_temp/\"\n",
        "shutil.rmtree(folder_data_temp, ignore_errors=True)\n",
        "\n",
        "path_save = current_dir + \"free.zip\"\n",
        "shutil.unpack_archive(path_save, folder_data_temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Custom PyTorch Datasets"
      ],
      "metadata": {
        "id": "UEwpfqtVWIqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `DGLDatasetReg` class is a custom dataset class specifically designed for regression tasks using Deep Graph Library (DGL). the `DGLDatasetReg` class provides a convenient way to handle regression datasets in DGL. It supports optional feature scaling and allows for customization through the `transform` parameter."
      ],
      "metadata": {
        "id": "Bgyv9p4wcd2H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "YmTs_DbbFTM-"
      },
      "outputs": [],
      "source": [
        "\"\"\" Regression Dataset \"\"\"\n",
        "class DGLDatasetReg(torch.utils.data.Dataset):\n",
        "    def __init__(self, address, transform=None, train=False, scaler=None, scaler_regression=None):\n",
        "            self.train = train\n",
        "            self.scaler = scaler\n",
        "            self.data_set, train_labels_masks_globals = dgl.load_graphs(address+\".bin\")\n",
        "            num_graphs = len(self.data_set)\n",
        "            self.labels = train_labels_masks_globals[\"labels\"].view(num_graphs,-1)\n",
        "            self.masks = train_labels_masks_globals[\"masks\"].view(num_graphs,-1)\n",
        "            self.globals = train_labels_masks_globals[\"globals\"].view(num_graphs,-1)\n",
        "            self.transform = transform\n",
        "            self.scaler_regression = scaler_regression\n",
        "    def scaler_method(self):\n",
        "        if self.train:\n",
        "            scaler = preprocessing.StandardScaler().fit(self.labels)\n",
        "            self.scaler = scaler\n",
        "        return self.scaler\n",
        "    def __len__(self):\n",
        "        return len(self.data_set)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.scaler_regression:\n",
        "            \"\"\" With Scaler\"\"\"\n",
        "            return  self.data_set[idx], torch.tensor(self.scaler.transform(self.labels)[idx]).float(), self.masks[idx], self.globals[idx]\n",
        "        else:\n",
        "            \"\"\" Without Scaler \"\"\"\n",
        "            return  self.data_set[idx], self.labels[idx].float(), self.masks[idx], self.globals[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining Train, Validation, and Test Set"
      ],
      "metadata": {
        "id": "f5hLJblRWKR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code snippet demonstrates the construction of training, validation, and test datasets using the DGLDatasetReg class. It also shows the use of a scaler to preprocess the data, ensuring that the validation and test sets are scaled consistently with the training set."
      ],
      "metadata": {
        "id": "WuEU4tkdc_H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "lzrwkfWAznp-"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_data_temp = folder_data_temp + \"scaffold\" + \"_\" + str(0)\n",
        "train_set = DGLDatasetReg(address=path_data_temp + \"_train\", train=True)\n",
        "scaler = train_set.scaler_method()\n",
        "val_set = DGLDatasetReg(address=path_data_temp + \"_val\", scaler=scaler)\n",
        "test_set = DGLDatasetReg(address=path_data_temp + \"_test\", scaler=scaler)\n",
        "\n",
        "print(len(train_set), len(val_set), len(test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo9Py1kOX5lK",
        "outputId": "4e7fdce9-eb31-43a2-f2aa-bc2e2f7883bd"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513 64 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "WJ-kbDA2VLqo"
      },
      "outputs": [],
      "source": [
        "# path_data_temp = folder_data_temp + \"scaffold\"+\"_\"+str(0)\n",
        "# train_data =DGLDatasetReg(address=path_data_temp+\"_train\")\n",
        "# val_data = DGLDatasetReg(address=path_data_temp+\"_val\")\n",
        "# test_data = DGLDatasetReg(address=path_data_temp+\"_test\")\n",
        "\n",
        "# print(len(train_data), len(val_data), len(test_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def scale_data(train_data, val_data, test_data, scaler):\n",
        "#     # Fit scaler on training data and transform training, validation, and test data\n",
        "#     train_features, train_labels = train_data\n",
        "#     val_features, val_labels = val_data\n",
        "#     test_features, test_labels = test_data\n",
        "\n",
        "#     scaler.fit(train_features)\n",
        "\n",
        "#     train_features_scaled = scaler.transform(train_features)\n",
        "#     val_features_scaled = scaler.transform(val_features)\n",
        "#     test_features_scaled = scaler.transform(test_features)\n",
        "\n",
        "#     train_data_scaled = (train_features_scaled, train_labels)\n",
        "#     val_data_scaled = (val_features_scaled, val_labels)\n",
        "#     test_data_scaled = (test_features_scaled, test_labels)\n",
        "\n",
        "#     return train_data_scaled, val_data_scaled, test_data_scaled"
      ],
      "metadata": {
        "id": "HrjD9mR7hNq7"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Loader"
      ],
      "metadata": {
        "id": "ZbDCjA5vWPWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code snippet defines a collate function that concatenates the graphs, labels, masks, and globals in a batch of data. It also defines a loader function that creates data loaders for the training, validation, and test sets using the collate function. The data loaders can be used to iterate over the data in batches during training and evaluation."
      ],
      "metadata": {
        "id": "78cnm5AudH_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "LSbrroSbgwtn"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "aVaK-lHgVLqo"
      },
      "outputs": [],
      "source": [
        "def collate(batch):\n",
        "    # batch is a list of tuples (graphs, labels, masks, globals)\n",
        "    # Concatenate a sequence of graphs\n",
        "    graphs = [e[0] for e in batch]\n",
        "    g = dgl.batch(graphs)\n",
        "\n",
        "    # Concatenate a sequence of tensors (labels) along a new dimension\n",
        "    labels = [e[1] for e in batch]\n",
        "    labels = torch.stack(labels, 0).float()  # Convert labels to float\n",
        "\n",
        "    # Concatenate a sequence of tensors (masks) along a new dimension\n",
        "    masks = [e[2] for e in batch]\n",
        "    masks = torch.stack(masks, 0)\n",
        "\n",
        "    # Concatenate a sequence of tensors (globals) along a new dimension\n",
        "    globals = [e[3] for e in batch]\n",
        "    globals = torch.stack(globals, 0)\n",
        "\n",
        "    return g, labels, masks, globals\n",
        "\n",
        "\n",
        "def loader(batch_size=64):\n",
        "    train_dataloader = DataLoader(train_set,\n",
        "                                  batch_size=batch_size,\n",
        "                                  collate_fn=collate,\n",
        "                                  drop_last=False,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=1)\n",
        "\n",
        "    val_dataloader = DataLoader(val_set,\n",
        "                                batch_size=batch_size,\n",
        "                                collate_fn=collate,\n",
        "                                drop_last=False,\n",
        "                                shuffle=False,\n",
        "                                num_workers=1)\n",
        "\n",
        "    test_dataloader = DataLoader(test_set,\n",
        "                                 batch_size=batch_size,\n",
        "                                 collate_fn=collate,\n",
        "                                 drop_last=False,\n",
        "                                 shuffle=False,\n",
        "                                 num_workers=1)\n",
        "    return train_dataloader, val_dataloader, test_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "FKDyB8i9VLqp"
      },
      "outputs": [],
      "source": [
        "train_dataloader, val_dataloader, test_dataloader = loader(batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining A GNN"
      ],
      "metadata": {
        "id": "UaZ4bwlMWYVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Some Variables"
      ],
      "metadata": {
        "id": "WcVkoB0wWaAO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "5IAAn7UrVLqp"
      },
      "outputs": [],
      "source": [
        "#Bace dataset has 1 task. Some other datasets may have some more number of tasks, e.g., tox21 has 12 tasks.\n",
        "num_tasks = 1\n",
        "\n",
        "# Size of global feature of each graph\n",
        "global_size = 200\n",
        "\n",
        "# Number of epochs to train the model\n",
        "num_epochs = 100\n",
        "\n",
        "# Number of steps to wait if the model performance on the validation set does not improve\n",
        "patience = 10\n",
        "\n",
        "#Configurations to instantiate the model\n",
        "config = {\"node_feature_size\":127, \"edge_feature_size\":12, \"hidden_size\":100}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the GNN class defines a GNN model with two graph convolution layers and a fully connected layer. It processes the input graph and produces predictions based on the node and global features. The model is designed for regression tasks andcan handle graphs with varying sizes and edge features."
      ],
      "metadata": {
        "id": "ZBLf86NPdtt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl.function as fn\n",
        "\n",
        "class SAGEConv(nn.Module):\n",
        "    \"\"\"Graph convolution module used by the GraphSAGE model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_feat : int\n",
        "        Input feature size.\n",
        "    out_feat : int\n",
        "        Output feature size.\n",
        "    aggregator_type : str\n",
        "        Aggregator type, e.g., 'mean', 'max', 'sum'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_feat, out_feat, aggregator_type='mean'):\n",
        "        super(SAGEConv, self).__init__()\n",
        "        self.aggregator_type = aggregator_type\n",
        "        self.linear = nn.Linear(in_feat * 2, out_feat)\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata[\"h\"] = h\n",
        "            g.update_all(\n",
        "                message_func=fn.copy_u(\"h\", \"m\"),\n",
        "                reduce_func=getattr(fn, self.aggregator_type)(\"m\", \"h_N\"),\n",
        "            )\n",
        "            h_N = g.ndata[\"h_N\"]\n",
        "            h_total = torch.cat([h, h_N], dim=1)\n",
        "            return self.linear(h_total)\n"
      ],
      "metadata": {
        "id": "FGhy58tF1hE_"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size=200, num_tasks=1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = SAGEConv(self.node_feature_size, self.hidden_size)\n",
        "        self.conv2 = SAGEConv(self.hidden_size, self.hidden_size)\n",
        "        self.conv3 = SAGEConv(self.hidden_size, self.hidden_size)\n",
        "        self.conv4 = SAGEConv(self.hidden_size, self.num_tasks)\n",
        "\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n",
        "        h = mol_dgl_graph.ndata[\"v\"]\n",
        "        h = self.conv1(mol_dgl_graph, h)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv3(mol_dgl_graph, h)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv4(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")\n"
      ],
      "metadata": {
        "id": "C1sm6VmSI5n4"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function to Compute Score of the Model"
      ],
      "metadata": {
        "id": "U3MbXZtxWmtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " the compute_score function evaluates the trained GNN model on a given dataset, calculates the RMSE score for the prediction tasks, and returns the score as a measure of the model's performance."
      ],
      "metadata": {
        "id": "K3UcfcfUeDLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks, scaler=None):\n",
        "    model.eval()\n",
        "    metric = mean_squared_error\n",
        "    with torch.no_grad():\n",
        "        prediction_all = torch.empty(0)\n",
        "        labels_all = torch.empty(0)\n",
        "        masks_all = torch.empty(0)\n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            if scaler is not None:\n",
        "                mol_dgl_graph.ndata['feat'] = scaler.transform(mol_dgl_graph.ndata['feat'])\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "        average = torch.tensor([0.])\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:, i] == 1]\n",
        "            a2 = labels_all[:, i][masks_all[:, i] == 1]\n",
        "            t = metric(a2.cpu(), a1.cpu()).item()\n",
        "            average += t\n",
        "    return (average.item() / num_tasks) ** 0.5  # Return the RMSE score\n"
      ],
      "metadata": {
        "id": "Kk0YryMLcc0K"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loss Function"
      ],
      "metadata": {
        "id": "Kx8IY1zjWqbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " the loss_func function calculates the MSE loss between the model's output and the target labels, applying a masking operation to handle missing or padded values. The loss is then averaged based on the number of valid elements in the batch and returned as the result."
      ],
      "metadata": {
        "id": "PFg74MxiexZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def loss_func(output, label, mask, num_tasks):\n",
        "    criterion = torch.nn.MSELoss(reduction='none')\n",
        "    loss = mask * criterion(output, label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "R3PNSMOUclai"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and Evaluation"
      ],
      "metadata": {
        "id": "ffP_01UoWt5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Function"
      ],
      "metadata": {
        "id": "Gb8uQ_TmWvcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train_epoch function encapsulates the training loop for one epoch. It performs the forward pass, loss calculation, backpropagation, and optimization steps for each batch in the training data. The average training loss is then returned, which can be used for monitoring the training progress."
      ],
      "metadata": {
        "id": "FkH9Xa6FfCXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train()  # Prepare model for training\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss\n"
      ],
      "metadata": {
        "id": "FS5kl8aUcx0y"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the code performs training and evaluation for a GNN model. It iterates over multiple epochs, updating the model based on the training loss and evaluating its performance on the validation set. The best model is saved based on the validation score, and the final results are printed."
      ],
      "metadata": {
        "id": "6tjzHVtNfeG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "def train_evaluate():\n",
        "    path_data_temp = folder_data_temp + \"scaffold\" + \"_\" + str(0)\n",
        "    train_set = DGLDatasetReg(address=path_data_temp + \"_train\", train=True)\n",
        "    scaler = train_set.scaler_method()\n",
        "    val_set = DGLDatasetReg(address=path_data_temp + \"_val\", scaler=scaler)\n",
        "    test_set = DGLDatasetReg(address=path_data_temp + \"_test\", scaler=scaler)\n",
        "\n",
        "    train_data = train_set  # Assign the train_set to train_data\n",
        "    val_data = val_set  # Assign the val_set to val_data\n",
        "    test_data = test_set  # Assign the test_set to test_data\n",
        "\n",
        "    model =  GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    best_val = float('inf')  # Initialize with infinity for regression\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_data), num_tasks)\n",
        "\n",
        "            if score_val < best_val:  # Update if score_val is lower (better) for regression\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "                epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "    # Best model save\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(best_val), \"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K8-AwJa5VZGA"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function to compute test set score of the final saved model"
      ],
      "metadata": {
        "id": "xANQIK43W-is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the test_evaluate function loads the best model checkpoint, evaluates the model on the test dataset, and reports the test score along with the execution time."
      ],
      "metadata": {
        "id": "onwT6ylrf8cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"
      ],
      "metadata": {
        "id": "H2IZQeRUc-Qw"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the model and evaluate its performance"
      ],
      "metadata": {
        "id": "p5VrUE2AW4sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " By calling train_evaluate() and test_evaluate() one after the other, the code performs both training and testing of the graph-based model. The start_time variable is used to calculate and print the total execution time for both operations."
      ],
      "metadata": {
        "id": "81j0hMPZgUKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgEKfu5gdHi_",
        "outputId": "56628128-4099-4261-89c5-c47a66aa0a63"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 23.449 | Valid Score: 6.827\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 6.827 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 2/100 | Training Loss: 27.452 | Valid Score: 6.796\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 6.796 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 3/100 | Training Loss: 30.655 | Valid Score: 6.758\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 6.758 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 4/100 | Training Loss: 23.325 | Valid Score: 6.706\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 6.706 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 5/100 | Training Loss: 21.145 | Valid Score: 6.632\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 6.632 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 6/100 | Training Loss: 20.395 | Valid Score: 6.525\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 6.525 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 7/100 | Training Loss: 25.025 | Valid Score: 6.364\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 6.364 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 19.547 | Valid Score: 6.124\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 6.124 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 9/100 | Training Loss: 16.386 | Valid Score: 5.806\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 5.806 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 18.938 | Valid Score: 5.376\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 5.376 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 11/100 | Training Loss: 12.608 | Valid Score: 4.886\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 4.886 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 12/100 | Training Loss: 13.117 | Valid Score: 4.461\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 4.461 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 13/100 | Training Loss: 12.288 | Valid Score: 4.210\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 4.210 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 14/100 | Training Loss: 10.962 | Valid Score: 4.206\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 4.206 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 15/100 | Training Loss: 15.829 | Valid Score: 4.219\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 4.206 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 16/100 | Training Loss: 10.464 | Valid Score: 4.155\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 4.155 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 17/100 | Training Loss: 11.229 | Valid Score: 4.221\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 4.155 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 18/100 | Training Loss: 10.521 | Valid Score: 4.324\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 4.155 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 19/100 | Training Loss: 10.319 | Valid Score: 4.410\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 4.155 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 20/100 | Training Loss: 9.964 | Valid Score: 4.428\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 4.155 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 21/100 | Training Loss: 10.514 | Valid Score: 4.401\n",
            " \n",
            "Epoch: 21/100 | Best Valid Score Until Now: 4.155 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 22/100 | Training Loss: 10.372 | Valid Score: 4.401\n",
            " \n",
            "Epoch: 22/100 | Best Valid Score Until Now: 4.155 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 23/100 | Training Loss: 12.520 | Valid Score: 4.363\n",
            " \n",
            "Epoch: 23/100 | Best Valid Score Until Now: 4.155 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 24/100 | Training Loss: 9.714 | Valid Score: 4.212\n",
            " \n",
            "Epoch: 24/100 | Best Valid Score Until Now: 4.155 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 25/100 | Training Loss: 9.544 | Valid Score: 4.172\n",
            " \n",
            "Epoch: 25/100 | Best Valid Score Until Now: 4.155 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 26/100 | Training Loss: 10.286 | Valid Score: 4.220\n",
            " \n",
            "Epoch: 26/100 | Best Valid Score Until Now: 4.155 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 4.155 \n",
            "\n",
            "Test Score: 4.066 \n",
            "\n",
            "Execution time: 25.062 seconds\n"
          ]
        }
      ]
    }
  ]
}