{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igOapoekt9YF",
        "outputId": "80576bd5-2ac3-40fd-9fff-d083ad86189c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-1.1.1-cp310-cp310-manylinux1_x86_64.whl (6.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/6.3 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S_zbs5_Jt3L5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl.function as fn\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "from torch.utils.data import DataLoader\n",
        "import cloudpickle\n",
        "from dgl.nn import GraphConv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jOl7-0ct3L5"
      },
      "source": [
        "#### Set Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this  segment involves directory and file operations. It creates directories, deletes directories, and extracts the contents of a ZIP file."
      ],
      "metadata": {
        "id": "dkDdNSjbu4Rz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cqFqDvR0t3L6"
      },
      "outputs": [],
      "source": [
        "current_dir = \"./\"\n",
        "checkpoint_path = current_dir + \"save_models/model_checkpoints/\" + \"checkpoint\"\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "best_model_path = current_dir + \"save_models/best_model/\"\n",
        "\n",
        "folder_data_temp = current_dir +\"data_temp/\"\n",
        "shutil.rmtree(folder_data_temp, ignore_errors=True)\n",
        "\n",
        "path_save = current_dir + \"hiv.zip\"\n",
        "shutil.unpack_archive(path_save, folder_data_temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCuniGpqt3L6"
      },
      "source": [
        "#### Custom PyTorch Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the `DGLDatasetClass` class represents a classification dataset that loads DGL graphs and associated labels, masks\n",
        "\n",
        ", and global information from a binary file. It allows for retrieving specific items from the dataset based on index."
      ],
      "metadata": {
        "id": "wRrC6aNWv5Wa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1z2DCX6Ft3L6"
      },
      "outputs": [],
      "source": [
        "\"\"\" Classification Dataset \"\"\"\n",
        "class DGLDatasetClass(torch.utils.data.Dataset):\n",
        "    def __init__(self, address):\n",
        "            self.address=address+\".bin\"\n",
        "            self.list_graphs, train_labels_masks_globals = dgl.load_graphs(self.address)\n",
        "            num_graphs =len(self.list_graphs)\n",
        "            self.labels = train_labels_masks_globals[\"labels\"].view(num_graphs,-1)\n",
        "            self.masks = train_labels_masks_globals[\"masks\"].view(num_graphs,-1)\n",
        "            self.globals = train_labels_masks_globals[\"globals\"].view(num_graphs,-1)\n",
        "    def __len__(self):\n",
        "        return len(self.list_graphs)\n",
        "    def __getitem__(self, idx):\n",
        "        return  self.list_graphs[idx], self.labels[idx], self.masks[idx], self.globals[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oHNkibPt3L6"
      },
      "source": [
        "#### Defining Train, Validation, and Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the code creates instances of the `DGLDatasetClass` class for training, validation, and test sets. The `address` parameter is set differently for each set to load the corresponding dataset. Finally, the code prints the lengths of the datasets to indicate the number of graphs in each set."
      ],
      "metadata": {
        "id": "8-96IBOVwvNp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQxBdhe_t3L7",
        "outputId": "12189eb5-322a-41bd-979f-189af2a51a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32901 4112 4114\n"
          ]
        }
      ],
      "source": [
        "path_data_temp = folder_data_temp + \"scaffold\"+\"_\"+str(0)\n",
        "train_set = DGLDatasetClass(address=path_data_temp+\"_train\")\n",
        "val_set = DGLDatasetClass(address=path_data_temp+\"_val\")\n",
        "test_set = DGLDatasetClass(address=path_data_temp+\"_test\")\n",
        "\n",
        "print(len(train_set), len(val_set), len(test_set))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBKR_7i_t3L7"
      },
      "source": [
        "#### Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the `collate` function is used to process a batch of samples by concatenating the graphs, labels, masks, and globals. The `loader` function creates dataloaders for the training, validation, and test sets using the `DataLoader` class, with the `collate` function specified to process the batches."
      ],
      "metadata": {
        "id": "4hcyWHEGxE66"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FMfrwxtQt3L7"
      },
      "outputs": [],
      "source": [
        "def collate(batch):\n",
        "    # batch is a list of tuples (graphs, labels, masks, globals)\n",
        "    # Concatenate a sequence of graphs\n",
        "    graphs = [e[0] for e in batch]\n",
        "    g = dgl.batch(graphs)\n",
        "\n",
        "    # Concatenate a sequence of tensors (labels) along a new dimension\n",
        "    labels = [e[1] for e in batch]\n",
        "    labels = torch.stack(labels, 0)\n",
        "\n",
        "    # Concatenate a sequence of tensors (masks) along a new dimension\n",
        "    masks = [e[2] for e in batch]\n",
        "    masks = torch.stack(masks, 0)\n",
        "\n",
        "    # Concatenate a sequence of tensors (globals) along a new dimension\n",
        "    globals = [e[3] for e in batch]\n",
        "    globals = torch.stack(globals, 0)\n",
        "\n",
        "    return g, labels, masks, globals\n",
        "\n",
        "\n",
        "def loader(batch_size=64):\n",
        "    train_dataloader = DataLoader(train_set,\n",
        "                              batch_size=batch_size,\n",
        "                              collate_fn=collate,\n",
        "                              drop_last=False,\n",
        "                              shuffle=True,\n",
        "                              num_workers=1)\n",
        "\n",
        "    val_dataloader =  DataLoader(val_set,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=collate,\n",
        "                             drop_last=False,\n",
        "                             shuffle=False,\n",
        "                             num_workers=1)\n",
        "\n",
        "    test_dataloader = DataLoader(test_set,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=collate,\n",
        "                             drop_last=False,\n",
        "                             shuffle=False,\n",
        "                             num_workers=1)\n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "btedvgv3t3L8"
      },
      "outputs": [],
      "source": [
        "train_dataloader, val_dataloader, test_dataloader = loader(batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4gHP6jet3L8"
      },
      "source": [
        "#### Defining A GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev4cykK0t3L8"
      },
      "source": [
        "##### Some Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " the code  sets up several parameters and configurations for training a model. These include the number of tasks, the size of global features, the number of epochs, the patience for early stopping, and the model configuration dictionary specifying the sizes of different features and hidden layers."
      ],
      "metadata": {
        "id": "kmaZaH2Ayjtf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v5ierpRkt3L8"
      },
      "outputs": [],
      "source": [
        "#Bace dataset has 1 task. Some other datasets may have some more number of tasks, e.g., tox21 has 12 tasks.\n",
        "num_tasks = 1\n",
        "\n",
        "# Size of global feature of each graph\n",
        "global_size = 200\n",
        "\n",
        "# Number of epochs to train the model\n",
        "num_epochs = 100\n",
        "\n",
        "# Number of steps to wait if the model performance on the validation set does not improve\n",
        "patience = 10\n",
        "\n",
        "#Configurations to instantiate the model\n",
        "config = {\"node_feature_size\":127, \"edge_feature_size\":12, \"hidden_size\":100}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " the `GNN` class represents a Graph Neural Network (GNN) model. It takes a DGL graph and global features as input and performs graph convolutions to obtain graph-level representations. The model architecture and parameters are defined in the `__init__` method, and the forward pass is implemented in the `forward` method."
      ],
      "metadata": {
        "id": "jaz2CTMDzEp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sUOiv0crt3L8"
      },
      "outputs": [],
      "source": [
        "# class GNN(nn.Module):\n",
        "#     def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "#         super().__init__()\n",
        "#         self.config = config\n",
        "#         self.num_tasks = num_tasks\n",
        "\n",
        "#         # Node feature size\n",
        "#         self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "#         # Edge feature size\n",
        "#         self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "#         # Hidden size\n",
        "#         self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "#         self.conv1 = GraphConv(self.node_feature_size, self.hidden_size,allow_zero_in_degree=True)\n",
        "#         self.conv2 = GraphConv(self.hidden_size, self.num_tasks,allow_zero_in_degree=True)\n",
        "\n",
        "#     # def forward(self, g, in_feat):\n",
        "#     def forward(self, mol_dgl_graph, globals):\n",
        "#         mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "#         mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "#         h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "#         h = F.relu(h)\n",
        "#         h = self.conv2(mol_dgl_graph, h)\n",
        "#         mol_dgl_graph.ndata[\"h\"] = h\n",
        "#         return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet defines a custom `SAGEConv` module that extends the `nn.Module` class from PyTorch. It implements the GraphSAGE convolutional operation by using DGL's message passing and reduce functions. The `forward` method defines the computation flow for the graph convolution layer."
      ],
      "metadata": {
        "id": "YZfjdj5iX2qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "from dgl.nn import GraphConv\n",
        "import dgl.function as fn\n",
        "from dgl.nn import SAGEConv\n",
        "\n",
        "\n",
        "class SAGEConv(nn.Module):\n",
        "    \"\"\"Graph convolution module used by the GraphSAGE model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_feat : int\n",
        "        Input feature size.\n",
        "    out_feat : int\n",
        "        Output feature size.\n",
        "    aggregator_type : str\n",
        "        Aggregator type, e.g., 'mean', 'max', 'sum'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_feat, out_feat, aggregator_type='mean'):\n",
        "        super(SAGEConv, self).__init__()\n",
        "        self.aggregator_type = aggregator_type\n",
        "        self.linear = nn.Linear(in_feat * 2, out_feat)\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata[\"h\"] = h\n",
        "            g.update_all(\n",
        "                message_func=fn.copy_u(\"h\", \"m\"),\n",
        "                reduce_func=getattr(fn, self.aggregator_type)(\"m\", \"h_N\"),\n",
        "            )\n",
        "            h_N = g.ndata[\"h_N\"]\n",
        "            h_total = torch.cat([h, h_N], dim=1)\n",
        "            return self.linear(h_total)"
      ],
      "metadata": {
        "id": "32aG_4T3V4U_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the `GNN` class implements a graph neural network model with two layers of SAGEConv graph convolutional layers. The node and edge features of the input graph are processed, and the output is obtained by aggregating the node features and computing the mean across the graph."
      ],
      "metadata": {
        "id": "1NXoe2KbX5I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size=200, num_tasks=1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = SAGEConv(self.node_feature_size, self.hidden_size, aggregator_type='mean')\n",
        "        self.conv2 = SAGEConv(self.hidden_size, self.num_tasks, aggregator_type='mean')\n",
        "\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")\n"
      ],
      "metadata": {
        "id": "9yJGr39mV5C_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVCgaSspt3L9"
      },
      "source": [
        "#### Function to Compute Score of the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the `compute_score` function evaluates the model by computing the ROC AUC score for each task in a multi-task setting. It aggregates the scores and returns the average score across all tasks."
      ],
      "metadata": {
        "id": "KKgTOhrizQEF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d3MV2-YLt3L9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "    model.eval()\n",
        "    metric = roc_auc_score\n",
        "    with torch.no_grad():\n",
        "        prediction_all= torch.empty(0)\n",
        "        labels_all= torch.empty(0)\n",
        "        masks_all= torch.empty(0)\n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction = torch.sigmoid(prediction)\n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "        average = torch.tensor([0.])\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:,i]==1]\n",
        "            a2 = labels_all[:, i][masks_all[:,i]==1]\n",
        "            try:\n",
        "                t = metric(a2.int().cpu(), a1.cpu()).item()\n",
        "            except ValueError:\n",
        "                t = 0\n",
        "            average += t\n",
        "    return average.item()/num_tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y18Wlm0et3L9"
      },
      "source": [
        "#### Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " the `loss_func` function calculates the loss for a multi-task binary classification problem using the binary cross-entropy loss (`BCEWithLogitsLoss`) with specified positive weights. It applies a mask to filter out padded or irrelevant elements, and the final loss is averaged over the valid elements."
      ],
      "metadata": {
        "id": "ufnBAhOh9TSg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uvfJ6Eh1t3L9"
      },
      "outputs": [],
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFfGjb_9t3L9"
      },
      "source": [
        "#### Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx1pNYqdt3L-"
      },
      "source": [
        "##### Training Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the `train_epoch` function performs one training epoch for a graph-based model. It iterates over the batches in the training data, computes the model's predictions and the training loss, performs backpropagation to compute the gradients, updates the model's parameters, and accumulates the training loss. The function returns the average training loss per iteration for the epoch."
      ],
      "metadata": {
        "id": "ue3kCM_990IX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C3j9TEy1t3L-"
      },
      "outputs": [],
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() # Prepare model for training\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the train_evaluate function trains and evaluates a graph-based model for a specified number of epochs. It keeps track of the best validation score and saves the corresponding model checkpoint. After training, it copies the checkpoint files of the best model to a separate directory. Finally, it prints the final results, including the average validation score."
      ],
      "metadata": {
        "id": "_bsiVcBT95Ou"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "g3_RC68Gt3L-"
      },
      "outputs": [],
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    # best model save\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQYY1p2qt3L-"
      },
      "source": [
        "##### Function to compute test set score of the final saved model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " the `test_evaluate` function loads the best model checkpoint, initializes a model with the same configuration, loads the model's state from the checkpoint, evaluates the model on the test dataset, and prints the test score and execution time."
      ],
      "metadata": {
        "id": "nQG4adnM_DyS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KOF0FfElt3L-"
      },
      "outputs": [],
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrhbK140t3L-"
      },
      "source": [
        "##### Train the model and evaluate its performance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By calling `train_evaluate()` and `test_evaluate()` one after the other, the code performs both training and testing of the graph-based model. The `start_time` variable is used to calculate and print the total execution time for both operations."
      ],
      "metadata": {
        "id": "MjDgu-Hm_YrX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIw5IQLyt3L-",
        "outputId": "71a44bc4-68dd-4713-8a1a-32f0a94e9248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.261 | Valid Score: 0.529\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.529 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 2/100 | Training Loss: 0.166 | Valid Score: 0.542\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.542 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 3/100 | Training Loss: 0.162 | Valid Score: 0.560\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.560 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 4/100 | Training Loss: 0.157 | Valid Score: 0.597\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.597 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 5/100 | Training Loss: 0.152 | Valid Score: 0.652\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.652 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 6/100 | Training Loss: 0.149 | Valid Score: 0.700\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.700 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 7/100 | Training Loss: 0.147 | Valid Score: 0.717\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.717 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 0.147 | Valid Score: 0.728\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.728 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 9/100 | Training Loss: 0.145 | Valid Score: 0.737\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.737 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 0.144 | Valid Score: 0.740\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.740 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 11/100 | Training Loss: 0.144 | Valid Score: 0.746\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.746 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 12/100 | Training Loss: 0.144 | Valid Score: 0.750\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 0.750 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 13/100 | Training Loss: 0.143 | Valid Score: 0.752\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 0.752 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 14/100 | Training Loss: 0.143 | Valid Score: 0.754\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 0.754 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 15/100 | Training Loss: 0.142 | Valid Score: 0.756\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 0.756 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 16/100 | Training Loss: 0.142 | Valid Score: 0.757\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 0.757 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 17/100 | Training Loss: 0.141 | Valid Score: 0.760\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 0.760 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 18/100 | Training Loss: 0.141 | Valid Score: 0.757\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 0.760 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 19/100 | Training Loss: 0.141 | Valid Score: 0.759\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 0.760 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 20/100 | Training Loss: 0.141 | Valid Score: 0.761\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 0.761 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 21/100 | Training Loss: 0.140 | Valid Score: 0.763\n",
            " \n",
            "Epoch: 21/100 | Best Valid Score Until Now: 0.763 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 22/100 | Training Loss: 0.140 | Valid Score: 0.765\n",
            " \n",
            "Epoch: 22/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 23/100 | Training Loss: 0.140 | Valid Score: 0.760\n",
            " \n",
            "Epoch: 23/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 24/100 | Training Loss: 0.140 | Valid Score: 0.762\n",
            " \n",
            "Epoch: 24/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 25/100 | Training Loss: 0.139 | Valid Score: 0.763\n",
            " \n",
            "Epoch: 25/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 26/100 | Training Loss: 0.139 | Valid Score: 0.762\n",
            " \n",
            "Epoch: 26/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 27/100 | Training Loss: 0.138 | Valid Score: 0.762\n",
            " \n",
            "Epoch: 27/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 28/100 | Training Loss: 0.139 | Valid Score: 0.763\n",
            " \n",
            "Epoch: 28/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 29/100 | Training Loss: 0.138 | Valid Score: 0.764\n",
            " \n",
            "Epoch: 29/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 30/100 | Training Loss: 0.138 | Valid Score: 0.761\n",
            " \n",
            "Epoch: 30/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 31/100 | Training Loss: 0.137 | Valid Score: 0.764\n",
            " \n",
            "Epoch: 31/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 32/100 | Training Loss: 0.137 | Valid Score: 0.763\n",
            " \n",
            "Epoch: 32/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.765 \n",
            "\n",
            "Test Score: 0.608 \n",
            "\n",
            "Execution time: 245.045 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}